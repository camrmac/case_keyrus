# Arquitetura

## VisÃ£o Geral

Este documento detalha a arquitetura end-to-end da pipeline do processamento de dados de APIs.

## Camadas da Arquitetura

### 1. Camada de Origem 

**Fonte de Dados:**

**APIs REST do IBGE (Se Habilitado em .yaml):** 
- PopulaÃ§Ã£o Municipal (Agregado 6579)
- PIB Municipal (Agregado 5938)
- Localidades (Metadados geogrÃ¡ficos)

**APIs REST do Banco Central do Brasil (BACEN):**
- IPCA Mensal (SÃ©rie 433)
- IPCA Acumulado 12 Meses (SÃ©rie 13522)
- PIB Mensal (SÃ©rie 4380)
- Taxa de Desemprego (SÃ©rie 24369)
- IGP-M (SÃ©rie 189)
- IGP-DI (SÃ©rie 190)

**InformaÃ§Ãµes extras:**
- Protocolo: HTTPS
- Formato: JSON
- AutenticaÃ§Ã£o: NÃ£o requerida (APIs pÃºblicas)

### 2. Camada Bronze

**Objetivo:** Armazenar dados brutos exatamente como recebidos da fonte

**CaracterÃ­sticas TÃ©cnicas:**
- Formato: Parquet (compressÃ£o Snappy)
- Particionamento: `ano`, `fonte`
- Schema: Schema-on-read (flexÃ­vel)
- RetenÃ§Ã£o: 90 dias

**Estrutura de Dados:**
```
bronze/
â”œâ”€â”€ populacao/
â”‚   â””â”€â”€ ano=2024/
â”‚       â””â”€â”€ fonte=populacao/
â”‚           â””â”€â”€ *.parquet
â”œâ”€â”€ pib/
â”‚   â””â”€â”€ ano=2021/
â”‚       â””â”€â”€ fonte=pib/
â”‚           â””â”€â”€ *.parquet
â”œâ”€â”€ localidades/
â”‚   â””â”€â”€ *.parquet
â””â”€â”€ bacen_*/
    â”œâ”€â”€ bacen_ipca_mensal/
    â”œâ”€â”€ bacen_ipca_12m/
    â”œâ”€â”€ bacen_pib_mensal/
    â”œâ”€â”€ bacen_desemprego/
    â”œâ”€â”€ bacen_igpm/
    â””â”€â”€ bacen_igpdi/
        â””â”€â”€ *.parquet
```

**OperaÃ§Ãµes:**
- IngestÃ£o via API Client (Python requests)
- ConversÃ£o JSON â†’ Pandas â†’ Spark DataFrame â†’ Parquet
- AdiÃ§Ã£o de metadados: timestamp de processamento, data de carga
- Modo: Overwrite

### 3. Camada Silver

**Objetivo:** Dados limpos, padronizados e validados

**TransformaÃ§Ãµes Aplicadas:**
- RemoÃ§Ã£o de nulos em colunas crÃ­ticas
- PadronizaÃ§Ã£o de tipos de dados
- NormalizaÃ§Ã£o de nomes (UPPER, TRIM)
- DeduplicaÃ§Ã£o por chave de negÃ³cio
- ValidaÃ§Ã£o de ranges numÃ©ricos
- Enriquecimento com cÃ³digos geogrÃ¡ficos
- Processamento de sÃ©ries temporais
- ConversÃ£o de formatos de data
- AgregaÃ§Ã£o de indicadores econÃ´micos

**CaracterÃ­sticas TÃ©cnicas:**
- Formato: Parquet (compressÃ£o Snappy)
- Particionamento: `ano`
- Schema: Schema-on-write (rÃ­gido)
- RetenÃ§Ã£o: 365 dias

**Qualidade de Dados:**
- Completeness: > 95%
- Validity: 100% em campos crÃ­ticos
- Uniqueness: Sem duplicatas
- Consistency: PadrÃµes estabelecidos

### 4. Camada Gold (DataMart)

**Objetivo:** Modelo dimensional otimizado para anÃ¡lises

**Modelo de Dados:**

#### DimensÃµes

**dim_municipio**
```sql
sk_municipio (PK)        BIGINT
codigo_municipio         STRING
nome_municipio           STRING
codigo_uf                STRING
sigla_uf                 STRING
nome_uf                  STRING
codigo_regiao            STRING
nome_regiao              STRING
sigla_regiao             STRING
```

**dim_tempo**
```sql
sk_tempo (PK)            BIGINT
ano                      INT
semestre                 INT
trimestre                INT
```

#### Fatos

**fact_indicadores_municipio**
```sql
sk_municipio (FK)        BIGINT
sk_tempo (FK)            BIGINT
codigo_municipio         STRING
ano                      INT
populacao_total          BIGINT
pib_total                DOUBLE
pib_per_capita           DOUBLE
yoy_pib                  DOUBLE
yoy_populacao            DOUBLE
share_pib_uf             DOUBLE
share_pib_brasil         DOUBLE
rank_pib_uf              INT
rank_pib_brasil          INT
```

**fact_indicadores_uf** (Agregado)
```sql
codigo_uf                STRING
sigla_uf                 STRING
nome_uf                  STRING
ano                      INT
qtd_municipios           BIGINT
populacao_total          BIGINT
pib_total                DOUBLE
pib_per_capita_medio     DOUBLE
yoy_pib                  DOUBLE
yoy_populacao            DOUBLE
share_pib_brasil         DOUBLE
rank_pib_brasil          INT
```

**fact_indicadores_brasil** (Agregado)
```sql
ano                      INT
qtd_ufs                  BIGINT
populacao_total          BIGINT
pib_total                DOUBLE
pib_per_capita           DOUBLE
yoy_pib                  DOUBLE
yoy_populacao            DOUBLE
ipca_mensal_medio        DOUBLE
ipca_12m_medio           DOUBLE
pib_mensal_medio         DOUBLE
desemprego_medio         DOUBLE
igpm_medio               DOUBLE
igpdi_medio              DOUBLE
```

**Granularidade:** 
- MunicÃ­pio: 1 registro por municÃ­pio-ano
- UF: 1 registro por UF-ano
- Brasil: 1 registro por ano

**MÃ©tricas Calculadas:**
- YoY (Year over Year): VariaÃ§Ã£o percentual ano sobre ano
- Share: ParticipaÃ§Ã£o percentual (UF e Brasil)
- Rank: PosiÃ§Ã£o no ranking (UF e Brasil)
- PIB Total: Calculado como PIB per capita Ã— PopulaÃ§Ã£o

## ğŸ”„ Pipeline ETL

### OrquestraÃ§Ã£o

```python
DataPipeline
â”œâ”€â”€ BronzeLoader
â”‚   â”œâ”€â”€ IBGEAPIClient
â”‚   â”‚   â”œâ”€â”€ get_populacao_municipios()
â”‚   â”‚   â”œâ”€â”€ get_pib_municipios()
â”‚   â”‚   â””â”€â”€ get_localidades_info()
â”‚   â”œâ”€â”€ BACENAPIClient
â”‚   â”‚   â”œâ”€â”€ get_bacen_series()
â”‚   â”‚   â””â”€â”€ parse_bacen_response()
â”‚   â””â”€â”€ write_parquet()
â”œâ”€â”€ SilverProcessor
â”‚   â”œâ”€â”€ process_populacao()
â”‚   â”œâ”€â”€ process_pib()
â”‚   â”œâ”€â”€ process_localidades()
â”‚   â””â”€â”€ process_bacen_indicadores()
â”œâ”€â”€ GoldBuilder
â”‚   â”œâ”€â”€ build_dim_municipio()
â”‚   â”œâ”€â”€ build_dim_tempo()
â”‚   â”œâ”€â”€ build_fact_indicadores_municipio()
â”‚   â”œâ”€â”€ build_fact_indicadores_uf()
â”‚   â”œâ”€â”€ build_fact_indicadores_brasil()
â”‚   â””â”€â”€ build_fact_indicadores_brasil_bacen_only() 
â”œâ”€â”€ QualityChecker
â”‚   â”œâ”€â”€ run_bronze_checks()
â”‚   â”œâ”€â”€ run_silver_checks()
â”‚   â””â”€â”€ run_gold_checks()
â””â”€â”€ SQLExporter (Delivery Layer)
    â”œâ”€â”€ export_table()
    â”œâ”€â”€ export_all_indicators()
    â””â”€â”€ create_tables_if_not_exists()
```

### Fluxo de ExecuÃ§Ã£o

```
1. BRONZE
   â”œâ”€ IngestÃ£o API PopulaÃ§Ã£o (IBGE)
   â”œâ”€ IngestÃ£o API PIB (IBGE)
   â”œâ”€ IngestÃ£o Localidades (IBGE)
   â”œâ”€ IngestÃ£o SÃ©ries BACEN (IPCA, PIB, Desemprego, IGP-M, IGP-DI)
   â”œâ”€ Write Parquet (particionado)
   â””â”€ Quality Checks Bronze
   
2. SILVER
   â”œâ”€ Read Bronze Parquet
   â”œâ”€ Limpeza e PadronizaÃ§Ã£o (IBGE)
   â”œâ”€ Processamento SÃ©ries Temporais (BACEN)
   â”œâ”€ DeduplicaÃ§Ã£o
   â”œâ”€ ValidaÃ§Ãµes
   â”œâ”€ Write Parquet (particionado)
   â””â”€ Quality Checks Silver
   
3. GOLD
   â”œâ”€ Read Silver Parquet
   â”œâ”€ Build DimensÃµes
   â”œâ”€ Build Fato MunicÃ­pio
   â”‚  â”œâ”€ Join DimensÃµes
   â”‚  â”œâ”€ Calcular YoY
   â”‚  â”œâ”€ Calcular Shares
   â”‚  â””â”€ Calcular Rankings
   â”œâ”€ Agregar Fato UF
   â”œâ”€ Agregar Fato Brasil (com indicadores BACEN)
   â”œâ”€ Write Parquet
   â””â”€ Quality Checks Gold
   
4. DELIVERY
   â”œâ”€ Exportar para Tabelas SQL
   â”‚  â”œâ”€ dim_municipio
   â”‚  â”œâ”€ dim_tempo
   â”‚  â”œâ”€ fact_indicadores_municipio
   â”‚  â”œâ”€ fact_indicadores_uf
   â”‚  â””â”€ fact_indicadores_brasil
   â””â”€ ValidaÃ§Ã£o de ExportaÃ§Ã£o
```

## â˜ï¸ Arquitetura Azure

### Componentes Azure

#### 1. Azure Data Factory (ADF)
**FunÃ§Ã£o:** OrquestraÃ§Ã£o e agendamento

**Pipelines:**
- Pipeline DiÃ¡rio: IngestÃ£o incremental
- Pipeline Mensal: Carga completa
- Pipeline de Qualidade: ValidaÃ§Ãµes pÃ³s-carga

#### 2. Azure Databricks
**FunÃ§Ã£o:** Processamento PySpark

**Clusters:**
- **Bronze Cluster:** 
- **Silver/Gold Cluster:**

#### 3. Azure Data Lake Storage Gen2 (ADLS)
**FunÃ§Ã£o:** Armazenamento Data Lake

**Containers:**
```
datalake/
â”œâ”€â”€ bronze/
â”œâ”€â”€ silver/
â””â”€â”€ gold/
```

#### 4. Azure Synapse Analytics
**FunÃ§Ã£o:** Query analytics e serveless SQL

#### 5. Camada Delivery (SQL Export)
**FunÃ§Ã£o:** ExportaÃ§Ã£o de indicadores para tabelas SQL

**Bancos de Dados Suportados:**
- SQL Server (local/Azure)
- PostgreSQL
- Azure SQL Database
- Azure Synapse Analytics (Dedicated Pool)
- MySQL

**Funcionalidades:**
- ExportaÃ§Ã£o automÃ¡tica apÃ³s Gold Layer
- ExportaÃ§Ã£o manual via script
- CriaÃ§Ã£o automÃ¡tica de tabelas
- Suporte a mÃºltiplos schemas
- Modos: overwrite, append, ignore, error

**Tabelas Exportadas:**
- `dim_municipio`
- `dim_tempo`
- `fact_indicadores_municipio`
- `fact_indicadores_uf`
- `fact_indicadores_brasil`

**IntegraÃ§Ã£o:**
- Power BI 
- AnÃ¡lises SQL ad-hoc
- RelatÃ³rios em tempo real

#### 6. Azure Key Vault
**FunÃ§Ã£o:** Gerenciamento de segredos

**Secrets:**
- Storage Account Keys
- Service Principal credentials
- Database credentials (SQL export)
- API tokens (se aplicÃ¡vel)

#### 7. Azure Monitor
**FunÃ§Ã£o:** Monitoramento e alertas

**MÃ©tricas:**
- Pipeline execution duration
- Data quality scores
- Error rates
- Resource utilization

**Alertas:**
- Pipeline failure
- Quality checks failed
- High error rate
- Resource threshold

### SeguranÃ§a

**Authentication & Authorization:**
- Service Principal com RBAC
- Managed Identity para Databricks
- Azure AD Integration

**Network Security:**
- VNet Integration
- Private Endpoints para Storage
- NSG Rules

**Data Protection:**
- Encryption at rest (Azure Storage)
- Encryption in transit (TLS 1.2+)
- Column-level security (Synapse)

## ğŸ”§ Tecnologias Utilizadas

### Bibliotecas Utilizadas
- **Python 3.9+**: Linguagem principal
- **PySpark 3.5**: Processamento distribuÃ­do
- **Parquet**: Formato columnar
- **Pandas**: ManipulaÃ§Ã£o de dados

### Cloud & Infrastructure
- **Azure Data Factory**: OrquestraÃ§Ã£o
- **Azure Databricks**: Processamento
- **ADLS Gen2**: Storage
- **Azure Synapse**: Analytics

### Data Quality & Monitoring
- **Great Expectations**: ValidaÃ§Ãµes
- **Python Logging**: Logs estruturados
- **Azure Monitor**: Monitoramento

### BI & Visualization
- **Power BI**: Dashboards

### OtimizaÃ§Ãµes

1. **Particionamento inteligente:** Por ano e fonte
2. **CompressÃ£o eficiente:** Snappy para Parquet
3. **Broadcast joins:** Para tabelas dimensÃ£o pequenas
4. **Adaptive Query Execution:** Habilitado no Spark
5. **Cache de dados:** Para anÃ¡lises iterativas

### KPIs de Monitoramento
- Pipeline success rate
- Data freshness (lag time)
- Quality check pass rate
- Query performance (P95)
- Cost per GB processed

---

## ğŸ“¦ Camada Delivery (SQL Export)

### Objetivo

Exportar indicadores da camada Gold para tabelas SQL, permitindo:
- Consultas SQL diretas
- IntegraÃ§Ã£o com ferramentas de BI (Power BI, Tableau)
- AnÃ¡lises ad-hoc
- RelatÃ³rios em tempo real

### Arquitetura

```
Gold Layer (Parquet)
        â†“
   SQLExporter
        â†“
   JDBC Connection
        â†“
   SQL Database
   â”œâ”€â”€ dim_municipio
   â”œâ”€â”€ dim_tempo
   â”œâ”€â”€ fact_indicadores_municipio
   â”œâ”€â”€ fact_indicadores_uf
   â””â”€â”€ fact_indicadores_brasil
```

### ConfiguraÃ§Ã£o

**Modos de ExportaÃ§Ã£o:**
- `overwrite`: Substitui dados existentes
- `append`: Adiciona novos dados
- `ignore`: Ignora se tabela existe
- `error`: Falha se tabela existe

### IntegraÃ§Ã£o no Pipeline

A exportaÃ§Ã£o SQL Ã© executada automaticamente apÃ³s a construÃ§Ã£o da camada Gold:

```python
# No pipeline completo
pipeline.run_full_pipeline()
# â†’ Bronze â†’ Silver â†’ Gold â†’ SQL Export (se habilitado)
```

### Uso Manual

```bash
# Exportar todas as tabelas
python scripts/export_to_sql.py

# Exportar tabela especÃ­fica
python scripts/export_to_sql.py --table fact_indicadores_municipio

# Criar tabelas primeiro
python scripts/export_to_sql.py --create-tables
```

---

